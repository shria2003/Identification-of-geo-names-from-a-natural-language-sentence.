{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'state_id', 'state_code', 'state_name', 'country_id',\n",
       "       'country_code', 'country_name', 'latitude', 'longitude', 'wikiDataId'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities= pd.read_csv(\"cities.csv\")\n",
    "countries= pd.read_csv(\"countries.csv\")\n",
    "states= pd.read_csv(\"states.csv\")\n",
    "\n",
    "cities.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'iso3', 'iso2', 'numeric_code', 'phone_code', 'capital',\n",
       "       'currency', 'currency_name', 'currency_symbol', 'tld', 'native',\n",
       "       'region', 'region_id', 'subregion', 'subregion_id', 'nationality',\n",
       "       'timezones', 'latitude', 'longitude', 'emoji', 'emojiU'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'country_id', 'country_code', 'country_name',\n",
       "       'state_code', 'type', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating merged dataset\n",
    "countries+city+states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  City Name         State Name Country Name  Latitude  \\\n",
      "0                 Ashkāsham         Badakhshan  Afghanistan      33.0   \n",
      "1                  Fayzabad         Badakhshan  Afghanistan      33.0   \n",
      "2                      Jurm         Badakhshan  Afghanistan      33.0   \n",
      "3                   Khandūd         Badakhshan  Afghanistan      33.0   \n",
      "4                 Rāghistān         Badakhshan  Afghanistan      33.0   \n",
      "...                     ...                ...          ...       ...   \n",
      "150568             Redcliff  Midlands Province     Zimbabwe     -20.0   \n",
      "150569             Shangani  Midlands Province     Zimbabwe     -20.0   \n",
      "150570             Shurugwi  Midlands Province     Zimbabwe     -20.0   \n",
      "150571    Shurugwi District  Midlands Province     Zimbabwe     -20.0   \n",
      "150572  Zvishavane District  Midlands Province     Zimbabwe     -20.0   \n",
      "\n",
      "        Longitude  \n",
      "0            65.0  \n",
      "1            65.0  \n",
      "2            65.0  \n",
      "3            65.0  \n",
      "4            65.0  \n",
      "...           ...  \n",
      "150568       30.0  \n",
      "150569       30.0  \n",
      "150570       30.0  \n",
      "150571       30.0  \n",
      "150572       30.0  \n",
      "\n",
      "[150573 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.merge(cities, states, left_on='state_id', right_on='id', suffixes=('_city', '_state'))\n",
    "merged_data = pd.merge(merged_data, countries, left_on='country_id_city', right_on='id', suffixes=('', '_country'))\n",
    "\n",
    "# # Select the desired fields for the new dataset\n",
    "new_dataset = merged_data[['name_city', 'name_state', 'country_name_city', 'latitude', 'longitude']]\n",
    "\n",
    "# # Rename the columns for better clarity\n",
    "new_dataset.columns = ['City Name', 'State Name', 'Country Name', 'Latitude', 'Longitude']\n",
    "\n",
    "# Display the new dataset\n",
    "print(new_dataset)\n",
    "# merged_data.country_name_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "City Name       0\n",
      "State Name      0\n",
      "Country Name    0\n",
      "Latitude        0\n",
      "Longitude       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City Name</th>\n",
       "      <th>State Name</th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ashkāsham</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fayzabad</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jurm</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Khandūd</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rāghistān</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150568</th>\n",
       "      <td>Redcliff</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150569</th>\n",
       "      <td>Shangani</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150570</th>\n",
       "      <td>Shurugwi</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150571</th>\n",
       "      <td>Shurugwi District</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150572</th>\n",
       "      <td>Zvishavane District</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150558 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  City Name         State Name Country Name  Latitude  \\\n",
       "0                 Ashkāsham         Badakhshan  Afghanistan      33.0   \n",
       "1                  Fayzabad         Badakhshan  Afghanistan      33.0   \n",
       "2                      Jurm         Badakhshan  Afghanistan      33.0   \n",
       "3                   Khandūd         Badakhshan  Afghanistan      33.0   \n",
       "4                 Rāghistān         Badakhshan  Afghanistan      33.0   \n",
       "...                     ...                ...          ...       ...   \n",
       "150568             Redcliff  Midlands Province     Zimbabwe     -20.0   \n",
       "150569             Shangani  Midlands Province     Zimbabwe     -20.0   \n",
       "150570             Shurugwi  Midlands Province     Zimbabwe     -20.0   \n",
       "150571    Shurugwi District  Midlands Province     Zimbabwe     -20.0   \n",
       "150572  Zvishavane District  Midlands Province     Zimbabwe     -20.0   \n",
       "\n",
       "        Longitude  \n",
       "0            65.0  \n",
       "1            65.0  \n",
       "2            65.0  \n",
       "3            65.0  \n",
       "4            65.0  \n",
       "...           ...  \n",
       "150568       30.0  \n",
       "150569       30.0  \n",
       "150570       30.0  \n",
       "150571       30.0  \n",
       "150572       30.0  \n",
       "\n",
       "[150558 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = new_dataset.isna().sum()\n",
    "\n",
    "# Print the count of missing values in each column\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# To drop rows with missing values (if desired)\n",
    "# new_dataset = new_dataset.dropna()\n",
    "\n",
    "# To fill missing values with a specific value (if desired)\n",
    "# new_dataset = new_dataset.fillna(some_value)\n",
    "\n",
    "# Check for duplicate entries\n",
    "duplicates = new_dataset.duplicated(subset=['City Name', 'State Name', 'Country Name', 'Latitude', 'Longitude'])\n",
    "\n",
    "# Remove duplicate entries if found\n",
    "new_dataset = new_dataset[~duplicates]\n",
    "new_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = pd.read_csv(\"query.csv\")\n",
    "query.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'sentence' column to string type\n",
    "query['sentence'] = query['sentence'].astype(str)\n",
    "\n",
    "# Apply word tokenization\n",
    "query['tokenized_text'] = query['sentence'].apply(word_tokenize)\n",
    "\n",
    "# # Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "query['processed_text'] = query['tokenized_text'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stemming or lemmatization if needed\n",
    "# Example for stemming:\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "query['processed_text'] = query['processed_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New York']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Function to filter out non-place names\n",
    "def filter_place_names(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    named_entities = ne_chunk(tagged_words)\n",
    "\n",
    "    place_names = []\n",
    "    for tagged_tree in named_entities:\n",
    "        if hasattr(tagged_tree, 'label') and tagged_tree.label() == 'GPE': # GPE stands for Geo-Political Entity\n",
    "            place_names.append(' '.join(c[0] for c in tagged_tree.leaves()))\n",
    "\n",
    "    return place_names\n",
    "\n",
    "# Example usage with a sample sentence\n",
    "sample_sentence = \"I visited New York and had a great time in the city.\"\n",
    "filtered_places = filter_place_names(sample_sentence)\n",
    "print(filtered_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential matches for 'Nwe York':\n",
      "New York (Similarity: 88)\n"
     ]
    }
   ],
   "source": [
    "# %pip install fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fuzzy_match(query, choices, threshold=70):\n",
    "    \"\"\"\n",
    "    Function to perform fuzzy matching on a query string against a list of choices.\n",
    "    :param query: The query string to be matched.\n",
    "    :param choices: List of strings to match against.\n",
    "    :param threshold: Minimum similarity threshold for a match. Default is 70.\n",
    "    :return: List of tuples containing the matched strings and their similarity scores.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for choice in choices:\n",
    "        similarity = fuzz.ratio(query, choice)\n",
    "        if similarity >= threshold:\n",
    "            matches.append((choice, similarity))\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "canonical_names = ['New York', 'Los Angeles', 'San Francisco', 'London', 'Paris', 'Berlin']\n",
    "query = 'Nwe York'\n",
    "\n",
    "matches = fuzzy_match(query, canonical_names)\n",
    "if matches:\n",
    "    print(f\"Potential matches for '{query}':\")\n",
    "    for match, score in matches:\n",
    "        print(f\"{match} (Similarity: {score})\")\n",
    "else:\n",
    "    print(f\"No matches found for '{query}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential matches for 'New York':\n",
      "'New York' found in the City column (Similarity: 100)\n",
      "'New York' found in the State column (Similarity: 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shree\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "# Sample data\n",
    "countries = ['United States', 'United Kingdom', 'Canada', 'Australia']\n",
    "cities = ['New York', 'London', 'Toronto', 'Sydney']\n",
    "states = ['New York', 'London', 'Ontario', 'New South Wales']\n",
    "\n",
    "def fuzzy_match_with_column(query, countries, cities, states, threshold=70):\n",
    "    \"\"\"\n",
    "    Function to perform fuzzy matching on a query string across different fields and indicate the specific column.\n",
    "    :param query: The query string to be matched.\n",
    "    :param countries: List of country names.\n",
    "    :param cities: List of city names.\n",
    "    :param states: List of state names.\n",
    "    :param threshold: Minimum similarity threshold for a match. Default is 70.\n",
    "    :return: A tuple indicating the identified word, its column, and the similarity score.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for col, values in {'Country': countries, 'City': cities, 'State': states}.items():\n",
    "        result = process.extractOne(query, values)\n",
    "        if result[1] >= threshold:\n",
    "            matches.append((result[0], col, result[1]))\n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "query = 'New York'\n",
    "matches = fuzzy_match_with_column(query, countries, cities, states)\n",
    "\n",
    "if matches:\n",
    "    print(f\"Potential matches for '{query}':\")\n",
    "    for match, col, score in matches:\n",
    "        print(f\"'{match}' found in the {col} column (Similarity: {score})\")\n",
    "else:\n",
    "    print(f\"No matches found for '{query}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pandas' from 'C:\\\\Users\\\\shree\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\pandas\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dir(pd)\n",
    "# list(pd)\n",
    "print(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
